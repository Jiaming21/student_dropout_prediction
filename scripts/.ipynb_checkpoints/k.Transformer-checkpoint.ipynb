{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713e6ee3-b645-4476-b889-a68713d68fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76f86110-10f6-471f-89bf-8e7706e05c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_datasets_info = torch.load('/root/autodl-tmp/data/saved_datasets.pth', weights_only=False)\n",
    "train_dataset = loaded_datasets_info['train_dataset']\n",
    "val_dataset = loaded_datasets_info['val_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69b14a4f-b3d7-4cb0-a4ee-9c020cda8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "loaded_train_dataset = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
    "loaded_val_dataset = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da8af3e-76cd-4a52-a09f-5dee1368fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/SWEdiff/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerEncoderClassification, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=31, nhead=31), \n",
    "            num_layers= 3,\n",
    "        ) \n",
    "        self.fc = nn.Linear(31, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  \n",
    "        x = self.transformer_encoder(x) \n",
    "        x = x.permute(1, 0, 2) \n",
    "        x = x.flatten(1) \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = \"cpu\"\n",
    "model = TransformerEncoderClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38520170-921b-4939-b2e9-27ab0f26ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 8.068671610372128\n",
      "Epoch 2/200, Loss: 6.931968050762417\n",
      "Epoch 3/200, Loss: 6.557649870597227\n",
      "Epoch 4/200, Loss: 6.37549812397298\n",
      "Epoch 5/200, Loss: 6.171367631165149\n",
      "Epoch 6/200, Loss: 6.022550545510698\n",
      "Epoch 7/200, Loss: 5.884446511852099\n",
      "Epoch 8/200, Loss: 5.770300618728374\n",
      "Epoch 9/200, Loss: 5.609863017635546\n",
      "Epoch 10/200, Loss: 5.554776443670169\n",
      "Epoch 11/200, Loss: 5.442614163461837\n",
      "Epoch 12/200, Loss: 5.368890631813076\n",
      "Epoch 13/200, Loss: 5.280738810744721\n",
      "Epoch 14/200, Loss: 5.180228521275296\n",
      "Epoch 15/200, Loss: 5.07194127774071\n",
      "Epoch 16/200, Loss: 4.977056043488639\n",
      "Epoch 17/200, Loss: 4.864525935793648\n",
      "Epoch 18/200, Loss: 4.804166077834661\n",
      "Epoch 19/200, Loss: 4.692165494942274\n",
      "Epoch 20/200, Loss: 4.755434109469487\n",
      "Epoch 21/200, Loss: 4.544979502780655\n",
      "Epoch 22/200, Loss: 4.4785221105995445\n",
      "Epoch 23/200, Loss: 4.437399952033364\n",
      "Epoch 24/200, Loss: 4.315806706839479\n",
      "Epoch 25/200, Loss: 4.1615761874082215\n",
      "Epoch 26/200, Loss: 4.06914263169785\n",
      "Epoch 27/200, Loss: 4.111468633353291\n",
      "Epoch 28/200, Loss: 4.050582990626122\n",
      "Epoch 29/200, Loss: 3.8930500688716174\n",
      "Epoch 30/200, Loss: 3.850195446952444\n",
      "Epoch 31/200, Loss: 3.8044514692883578\n",
      "Epoch 32/200, Loss: 3.7464416656093515\n",
      "Epoch 33/200, Loss: 3.6562566137251027\n",
      "Epoch 34/200, Loss: 3.6645992484981895\n",
      "Epoch 35/200, Loss: 3.621456342328348\n",
      "Epoch 36/200, Loss: 3.4572217717278195\n",
      "Epoch 37/200, Loss: 3.315987946330384\n",
      "Epoch 38/200, Loss: 3.2343176583076807\n",
      "Epoch 39/200, Loss: 3.336656226774359\n",
      "Epoch 40/200, Loss: 3.1810167292242024\n",
      "Epoch 41/200, Loss: 3.160611823489082\n",
      "Epoch 42/200, Loss: 3.121959830457628\n",
      "Epoch 43/200, Loss: 3.0964746814334556\n",
      "Epoch 44/200, Loss: 2.99716822083263\n",
      "Epoch 45/200, Loss: 2.9166325391542856\n",
      "Epoch 46/200, Loss: 2.9380756247989326\n",
      "Epoch 47/200, Loss: 2.7753242385296684\n",
      "Epoch 48/200, Loss: 2.799366267825492\n",
      "Epoch 49/200, Loss: 2.681554254161879\n",
      "Epoch 50/200, Loss: 2.6259515963416256\n",
      "Epoch 51/200, Loss: 2.5900135493774044\n",
      "Epoch 52/200, Loss: 2.541560768073455\n",
      "Epoch 53/200, Loss: 2.6059755646521157\n",
      "Epoch 54/200, Loss: 2.472461899742484\n",
      "Epoch 55/200, Loss: 2.30239154512512\n",
      "Epoch 56/200, Loss: 2.4505389566408944\n",
      "Epoch 57/200, Loss: 2.4791720040159984\n",
      "Epoch 58/200, Loss: 2.3423359881043955\n",
      "Epoch 59/200, Loss: 2.16956910751411\n",
      "Epoch 60/200, Loss: 2.213602243634748\n",
      "Epoch 61/200, Loss: 2.1672203284699196\n",
      "Epoch 62/200, Loss: 2.035361766483661\n",
      "Epoch 63/200, Loss: 1.995460730213977\n",
      "Epoch 64/200, Loss: 2.0959589708993422\n",
      "Epoch 65/200, Loss: 2.102816587344545\n",
      "Epoch 66/200, Loss: 2.048099733893465\n",
      "Epoch 67/200, Loss: 1.9682259830063866\n",
      "Epoch 68/200, Loss: 1.9297025450101815\n",
      "Epoch 69/200, Loss: 1.922212096184178\n",
      "Epoch 70/200, Loss: 1.9578308957264663\n",
      "Epoch 71/200, Loss: 1.88747955934391\n",
      "Epoch 72/200, Loss: 1.7953217103183094\n",
      "Epoch 73/200, Loss: 1.9132211149192526\n",
      "Epoch 74/200, Loss: 1.7894715246410746\n",
      "Epoch 75/200, Loss: 1.6702999692820273\n",
      "Epoch 76/200, Loss: 1.630919249951883\n",
      "Epoch 77/200, Loss: 1.704498169522741\n",
      "Epoch 78/200, Loss: 1.7253372261564592\n",
      "Epoch 79/200, Loss: 1.6595694330462039\n",
      "Epoch 80/200, Loss: 1.5065117662694016\n",
      "Epoch 81/200, Loss: 1.5583485266928772\n",
      "Epoch 82/200, Loss: 1.5133456412137898\n",
      "Epoch 83/200, Loss: 1.4383666453647495\n",
      "Epoch 84/200, Loss: 1.295139671088495\n",
      "Epoch 85/200, Loss: 1.5630143935643834\n",
      "Epoch 86/200, Loss: 1.548392038658235\n",
      "Epoch 87/200, Loss: 1.4839018106245734\n",
      "Epoch 88/200, Loss: 1.3216046431560975\n",
      "Epoch 89/200, Loss: 1.4728173764424717\n",
      "Epoch 90/200, Loss: 1.384707633607674\n",
      "Epoch 91/200, Loss: 1.2940538185485038\n",
      "Epoch 92/200, Loss: 1.192651579390439\n",
      "Epoch 93/200, Loss: 1.2406563544367817\n",
      "Epoch 94/200, Loss: 1.2755170921125476\n",
      "Epoch 95/200, Loss: 1.0874542004027483\n",
      "Epoch 96/200, Loss: 1.2354033214663889\n",
      "Epoch 97/200, Loss: 1.1581658827636032\n",
      "Epoch 98/200, Loss: 1.1969544484244061\n",
      "Epoch 99/200, Loss: 1.2188843329981256\n",
      "Epoch 100/200, Loss: 1.1554174050656028\n",
      "Epoch 101/200, Loss: 1.137541211439205\n",
      "Epoch 102/200, Loss: 1.2744420859471957\n",
      "Epoch 103/200, Loss: 1.10141631670294\n",
      "Epoch 104/200, Loss: 1.125112514366533\n",
      "Epoch 105/200, Loss: 1.0277680649238898\n",
      "Epoch 106/200, Loss: 1.0734201992498775\n",
      "Epoch 107/200, Loss: 1.0620536082598708\n",
      "Epoch 108/200, Loss: 1.016003715108308\n",
      "Epoch 109/200, Loss: 0.9386346700333551\n",
      "Epoch 110/200, Loss: 1.0105677947987621\n",
      "Epoch 111/200, Loss: 0.8978417607401094\n",
      "Epoch 112/200, Loss: 0.9119702678553289\n",
      "Epoch 113/200, Loss: 0.9912236604569108\n",
      "Epoch 114/200, Loss: 0.9175704859455598\n",
      "Epoch 115/200, Loss: 0.9540854768382616\n",
      "Epoch 116/200, Loss: 0.9585800902908166\n",
      "Epoch 117/200, Loss: 0.8988734776185786\n",
      "Epoch 118/200, Loss: 0.8414018183779756\n",
      "Epoch 119/200, Loss: 0.93297580659468\n",
      "Epoch 120/200, Loss: 0.8613246419879801\n",
      "Epoch 121/200, Loss: 0.8262779082027667\n",
      "Epoch 122/200, Loss: 0.8968065134933942\n",
      "Epoch 123/200, Loss: 0.7929395100230512\n",
      "Epoch 124/200, Loss: 0.7731623640169435\n",
      "Epoch 125/200, Loss: 0.816200637746334\n",
      "Epoch 126/200, Loss: 0.6680175514640373\n",
      "Epoch 127/200, Loss: 0.8650014701719773\n",
      "Epoch 128/200, Loss: 0.8719733508333205\n",
      "Epoch 129/200, Loss: 0.6664845670572868\n",
      "Epoch 130/200, Loss: 0.7378256420513006\n",
      "Epoch 131/200, Loss: 0.9762943897673596\n",
      "Epoch 132/200, Loss: 0.8640307105016574\n",
      "Epoch 133/200, Loss: 0.685214256954737\n",
      "Epoch 134/200, Loss: 0.7195505563677365\n",
      "Epoch 135/200, Loss: 0.7728091564476032\n",
      "Epoch 136/200, Loss: 0.8099689150915592\n",
      "Epoch 137/200, Loss: 0.7427528482767485\n",
      "Epoch 138/200, Loss: 0.843993637650012\n",
      "Epoch 139/200, Loss: 0.7081239908266773\n",
      "Epoch 140/200, Loss: 0.6876738872122787\n",
      "Epoch 141/200, Loss: 0.723367924777468\n",
      "Epoch 142/200, Loss: 0.6265426991256313\n",
      "Epoch 143/200, Loss: 0.7362923387285928\n",
      "Epoch 144/200, Loss: 0.6797647232051275\n",
      "Epoch 145/200, Loss: 0.6247294140377532\n",
      "Epoch 146/200, Loss: 0.5910470779203816\n",
      "Epoch 147/200, Loss: 0.6447137101242303\n",
      "Epoch 148/200, Loss: 0.664919112064435\n",
      "Epoch 149/200, Loss: 0.6207128948704077\n",
      "Epoch 150/200, Loss: 0.6993989322332619\n",
      "Epoch 151/200, Loss: 0.605886111184393\n",
      "Epoch 152/200, Loss: 0.6122686971560779\n",
      "Epoch 153/200, Loss: 0.6224490094781893\n",
      "Epoch 154/200, Loss: 0.4637172804541904\n",
      "Epoch 155/200, Loss: 0.6199436414636103\n",
      "Epoch 156/200, Loss: 0.6170962949649297\n",
      "Epoch 157/200, Loss: 0.5700167375830748\n",
      "Epoch 158/200, Loss: 0.6037126208918083\n",
      "Epoch 159/200, Loss: 0.5718272804961195\n",
      "Epoch 160/200, Loss: 0.4900587453285098\n",
      "Epoch 161/200, Loss: 0.4200830831582575\n",
      "Epoch 162/200, Loss: 0.49797763820840923\n",
      "Epoch 163/200, Loss: 0.47850716683763594\n",
      "Epoch 164/200, Loss: 0.6926396343995684\n",
      "Epoch 165/200, Loss: 0.489574575478626\n",
      "Epoch 166/200, Loss: 0.530857048262553\n",
      "Epoch 167/200, Loss: 0.4937304017952326\n",
      "Epoch 168/200, Loss: 0.5523587042083162\n",
      "Epoch 169/200, Loss: 0.49258142222191775\n",
      "Epoch 170/200, Loss: 0.5149294125323512\n",
      "Epoch 171/200, Loss: 0.3547428030707024\n",
      "Epoch 172/200, Loss: 0.367390316395011\n",
      "Epoch 173/200, Loss: 0.5156024101703368\n",
      "Epoch 174/200, Loss: 0.5187763629430213\n",
      "Epoch 175/200, Loss: 0.5398613341919182\n",
      "Epoch 176/200, Loss: 0.5654760599436933\n",
      "Epoch 177/200, Loss: 0.5102403456521323\n",
      "Epoch 178/200, Loss: 0.4446063826429882\n",
      "Epoch 179/200, Loss: 0.4731745921402915\n",
      "Epoch 180/200, Loss: 0.5047600504964594\n",
      "Epoch 181/200, Loss: 0.531646488848512\n",
      "Epoch 182/200, Loss: 0.4999413290502044\n",
      "Epoch 183/200, Loss: 0.3989714548747361\n",
      "Epoch 184/200, Loss: 0.4036723650172374\n",
      "Epoch 185/200, Loss: 0.3955132584862857\n",
      "Epoch 186/200, Loss: 0.4182682167652114\n",
      "Epoch 187/200, Loss: 0.326542503178865\n",
      "Epoch 188/200, Loss: 0.4879305418938604\n",
      "Epoch 189/200, Loss: 0.48871948471826804\n",
      "Epoch 190/200, Loss: 0.3941217510643648\n",
      "Epoch 191/200, Loss: 0.5292004772617148\n",
      "Epoch 192/200, Loss: 0.3328171117506339\n",
      "Epoch 193/200, Loss: 0.31914216758734715\n",
      "Epoch 194/200, Loss: 0.4207595548006955\n",
      "Epoch 195/200, Loss: 0.41669345026387583\n",
      "Epoch 196/200, Loss: 0.4919656463014698\n",
      "Epoch 197/200, Loss: 0.40265517722214383\n",
      "Epoch 198/200, Loss: 0.45359522852630474\n",
      "Epoch 199/200, Loss: 0.3197887419311022\n",
      "Epoch 200/200, Loss: 0.3869641516210437\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_train_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / (len(loaded_train_dataset) / batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b28ddac4-3e10-4d15-8674-44b93bef0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/root/autodl-tmp/model_params/Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd86c85-7aee-4a0c-81ce-a2270bf537d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/root/autodl-tmp/model_params/Transformer.pth', weights_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6bc800-e4fb-426b-b73d-d17f922c2968",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393bebac-2b67-4e62-ae91-a66e60c5350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC\n",
    "def calculate_multiclass_metrics(true_labels, predicted_labels, predicted_probabilities, num_classes):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "    \n",
    "    sensitivity_per_class = []\n",
    "    specificity_per_class = []\n",
    "    auc_per_class = []\n",
    "    f1_per_class = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        true_binary = (np.array(true_labels) == i).astype(int)\n",
    "        pred_binary = (np.array(predicted_labels) == i).astype(int)\n",
    "\n",
    "        cm = confusion_matrix(true_binary, pred_binary, labels=[0, 1])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity_per_class.append(sensitivity)\n",
    "        specificity_per_class.append(specificity)\n",
    "\n",
    "        \n",
    "        auc = roc_auc_score(true_binary, predicted_probabilities[:, i]) if len(np.unique(true_binary)) > 1 else 0\n",
    "        auc_per_class.append(auc)\n",
    "\n",
    "        f1 = f1_score(true_binary, pred_binary) if len(np.unique(true_binary)) > 1 else 0\n",
    "        f1_per_class.append(f1)\n",
    "\n",
    "    avg_sensitivity = np.mean(sensitivity_per_class)\n",
    "    avg_specificity = np.mean(specificity_per_class)\n",
    "    avg_auc = np.mean(auc_per_class) if auc_per_class else 0\n",
    "    avg_f1 = np.mean(f1_per_class)\n",
    "\n",
    "    print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "    print(f\"Average Sensitivity: {avg_sensitivity:.4f}\")\n",
    "    print(f\"Average Specificity: {avg_specificity:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Average F1-score: {avg_f1:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d903046-e9ac-4f7d-b1ab-9a6cd1134775",
   "metadata": {},
   "source": [
    "### Training data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af43577a-382a-4261-bbc9-758d716ac020",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = []\n",
    "true_labels = []\n",
    "with torch.set_grad_enabled(False):\n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_train_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)      \n",
    "        outputs = model(inputs)\n",
    "        predicted_probabilities.extend(outputs.tolist())\n",
    "        true_labels.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d634c59a-070e-4670-adef-ff81574ee427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC: 0.9996\n",
      "Average Sensitivity: 0.9842\n",
      "Average Specificity: 0.9921\n",
      "Accuracy: 0.9841\n",
      "Average F1-score: 0.9841\n",
      "Matthews Correlation Coefficient (MCC): 0.9762\n"
     ]
    }
   ],
   "source": [
    "true_labels_ = np.argmax(true_labels, axis=-1)\n",
    "predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
    "preds = torch.tensor(predicted_probabilities)\n",
    "preds = F.softmax(preds, dim=-1)\n",
    "\n",
    "train_metrics = calculate_multiclass_metrics(true_labels_, predicted_labels, preds, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e90d282-a10c-43bf-b53e-48dcf08073b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/autodl-tmp/ROC/Transformer/y_train_pred.npy', preds)\n",
    "np.save('/root/autodl-tmp/ROC/Transformer/y_train.npy', true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf505ac-2ace-48e8-b275-74cbbb100296",
   "metadata": {},
   "source": [
    "### Validation data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa633154-87be-4a95-9990-ed59878f005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = []  \n",
    "true_labels = []  \n",
    "with torch.set_grad_enabled(False): \n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_val_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        outputs = model(inputs)\n",
    "        predicted_probabilities.extend(outputs.tolist())\n",
    "        true_labels.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc65513-5e8b-4737-bbbf-b6c99e78ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_ = np.argmax(true_labels, axis=-1)\n",
    "predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
    "preds = torch.tensor(predicted_probabilities)\n",
    "preds = F.softmax(preds, dim=-1)\n",
    "\n",
    "test_metrics = calculate_multiclass_metrics(true_labels_, predicted_labels, preds, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a3d871f-f2b4-4f8e-9e4c-b0a87e3d879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/autodl-tmp/ROC/Transformer/y_val_pred.npy', preds)\n",
    "np.save('/root/autodl-tmp/ROC/Transformer/y_val.npy', true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ab260-98f6-4613-87cc-4ee537b7356d",
   "metadata": {},
   "source": [
    "### Testing data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b513e60c-3c9e-4048-87f3-d3af20a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "test_data_new = pd.read_csv(\"/root/autodl-tmp/data/test_data_new.csv\")\n",
    "features = test_data_new.iloc[:, :]\n",
    "test_data_input = torch.tensor(features.values, dtype=torch.float32)\n",
    "test_data_input = test_data_input.unsqueeze(1)\n",
    "test_data_output = model(test_data_input)\n",
    "torch.save(test_data_output, \"/root/autodl-tmp/data/test_data_output.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de24ada-a0a2-410e-8cb7-40e9518bf7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEdiff",
   "language": "python",
   "name": "swediff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
